\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{fullpage}
\usepackage{commath}
\usepackage{graphicx}
\usepackage{pdfcomment}
%\usepackage{coffee4}
\usepackage{lipsum}
\usepackage{showkeys}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{verbatim}
\usepackage{longtable}

%General Shorthand Macros
\newcommand{\skipLine}{\vspace{12pt}}
\newcommand{\mb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\ms}{\mathscr}
\newcommand{\ra}{\rightarrow}
\newcommand{\ov}{\overline}
\newcommand{\os}{\overset}
\newcommand{\un}{\underline}
\newcommand{\te}{\text}
\newcommand{\ep}{\epsilon}
\newcommand{\tr}{\textcolor{red}}
\newcommand{\tb}{\textcolor{blue}}
\newcommand{\tg}{\textcolor{green}}
\newcommand{\labe}[1]{\tr{\texttt{Label: #1}}}
\newcommand{\tbs}{\textbackslash}
\newcommand{\purpose}{\textbf{Purpose: }}
\newcommand{\pfsum}{\textbf{Proof Summary: }}
\newcommand{\usein}{\textbf{Used in: }}
\newcommand{\app}{\textbf{Applies: }}
\newcommand{\ind}{\hspace{24pt}}
\newcommand{\lin}{\rule{\linewidth}{0.4 pt}}
\newcommand{\pr}{\mb{P}}							%probability
\newcommand{\ex}[1]{\mb{E}\left[#1\right]}			%expectation
\newcommand{\exmu}[2]{\mb{E}^{#1}\left[#2\right]}	%exp wrt a measure
\newcommand{\deq}{\overset{\text{(d)}}{=}}			%equal in dist
\newcommand{\defeq}{:=}								%definition equal
\newcommand{\msr}{\mc{M}}							%space of measures
\newcommand{\pmsr}{\mc{P}}							%space of pmsrs
\newcommand{\cad}{\mb{D}}							%Cadlag space
\newcommand{\argmin}{\te{arg}\min}


%Notation and Basic Assumptions
%Graph Notation
%Base Commands
\newcommand{\sta}{\mc{X}}							%state space
\newcommand{\neigh}[1]{\partial_{#1}}				%neighborhood
\newcommand{\dneigh}[1]{\partial^2_{#1}}			%double neigh
\newcommand{\cl}[1]{\ov{#1}}						%graph closure
\renewcommand{\root}{\mathbf{0}}

%Modifiers
\newcommand{\stb}[1]{_{#1}}							%add base of \st
\newcommand{\indx}[1]{^{#1}}						%sublimit index
\newcommand{\subg}[1]{_{#1}}						%subgraph


%Process Notation
%Base Commands
\newcommand{\Xf}{X}									%Full process
\newcommand{\poiss}{N}								%Poisson process
\newcommand{\leb}{\lambda}							%Lebesgue msr
\newcommand{\Sm}{\ell}								%ctng msr on sta
\newcommand{\rate}{r}								%jump rate
\newcommand{\F}{\mc{F}}								%filtrations
\newcommand{\m}{\mu}								%law of \Xf
\newcommand{\proj}{\pi}								%projection

%Modifiers
\newcommand{\poissv}[1]{_{#1}}						%v comp of Poisson
\newcommand{\poisso}[1]{^{#1}}						%Other P modifier
\newcommand{\vind}[1]{_{#1}}						%v component
\newcommand{\tme}[1]{(#1)}							%time
\newcommand{\tmi}[1]{#1}							%time interval
\newcommand{\gind}[1]{^{#1}}						%interaction net
\newcommand{\vpara}[1]{^{#1}}						%vertex param
\newcommand{\stpara}[1]{_{#1}}						%state parameter	
\newcommand{\tpara}[1]{_{#1}}						%time parameter
\newcommand{\gvpara}[2]{^{#1,#2}}					%G and v params
\newcommand{\psf}{_*}								%push forward
\newcommand{\tparapsf}[1]{_{#1,*}}					%psf t param


%Simultaneous Jumps
\newcommand{\Jmps}{\mc{J}}							%set of jumps


%Assumptions


%Well-Posedness
%Base Commands

%Modifiers
\newcommand{\trnc}[1]{_{#1}}						%Truncated graph




%Statement
\newcommand{\intA}{A^\mathrm{o}}

%Existence
\newcommand{\pmap}{\Lambda}
\newcommand{\rt}{\tau}
\renewcommand{\mark}{\kappa}
\newcommand{\Xg}{Y}
\newcommand{\ratee}{\Gamma}
\newcommand{\rp}{P}

\newcommand{\alt}[1]{\tilde{#1}}
\newcommand{\cratee}{\alt{\ratee}}
\newcommand{\law}{\te{Law}}
\newcommand{\Xh}{Z}





%reassign later
\newcommand{\arr}{\lambda}							%arrival rate
\newcommand{\neighI}[1]{\partial^I_{#1}}			%int. neigh
\newcommand{\IG}{\mc{L}}							%infinitesimal gen
\newcommand{\ev}[1]{\ep^{#1}}						%std basis
\newcommand{\para}[1]{^{#1}}
\newcommand{\inter}[1]{#1^I}
\newcommand{\uni}{m}
\renewcommand{\d}{D}


\newtheorem{thms}{Theorem}[section]
\newtheorem{conj}[thms]{Conjecture}
\newtheorem{prop}[thms]{Proposition}
\newtheorem{coro}[thms]{Corollary}
\newtheorem{lem}[thms]{Lemma}
%\newtheorem{sublem}{Sublemma}[lem]
\newtheorem{defn}[thms]{Definition}
\newtheorem{assu}[thms]{Assumption}

\setlength{\parindent}{0pt}

\begin{document}

\title{General Graph Topology Results (Working Title)}
\author{Ankan Ganguly}

\maketitle

Remark: This document uses the results from the derivation of the local approximation on trees. I will refer to this derivation as the main paper for now.

\skipLine

Remark: In this paper I mostly work with locally finite graphs. However, I proved all my results in the main paper for bounded degree graphs. I may need to strengthen the assumptions of this paper to bounded degree. However, since we are working with local convergence, all results that hold for locally finite graphs should extend to bounded degree graphs (the one possible exception would be well-posedness results).

\section{Notation and basic assumptions}
\label{not}

\subsection{Graph Notation}
\label{g::not}

We consider an interacting particle system for which each node takes values in the countable state-space \(\sta = \mb{Z}\). Our goal is to understand the local evolution of a network whose nodes take values in \(\sta\). Therefore, we represent the interaction network between nodes by a rooted graph \(G = (V,E,\root)\) in which \(\root \in V\) is the vertex representing the node whose local evolution is of interest to us. When looking at sequences of such processes on networks, we let \(G\indx{k} = (V\indx{k},E\indx{k},\root\indx{k})\) represent the network structure in the sublimit.

\ind Given a specific rooted graph \(G\) and any vertex set \(\root \in A \subseteq V\), define \(G\subg{A} \defeq (A,E\cap A^2,\root)\). This is the maximal subgraph of \(G\) restricted to the vertices in \(A\). For any \(v \in V\), let \(\neigh{v}\subseteq V\) be the neighbors of \(v\) in \(g\). Let \(\cl{v} = \{v\}\cup\neigh{v}\). We also define the double neighborhood given by, \(\dneigh{v} = \neigh{v} \cup \neigh{\neigh{v}}\setminus \{v\}\). These notions also extend to vertex set. If \(A\subseteq V\), then \(\neigh{A} = \{v \in V\setminus A: \exists u \in A\te{ s.t. } (u,v) \in E\}\). \(\cl{A} = A\cup \neigh{A}\). \(\dneigh{A} = \neigh{A} \cup\neigh{\neigh{A}}\setminus A\).

\subsection{Process Notation}
\label{p::not}

In this context, define the \(\sta^V\)-valued c\`adl\`ag process \(\Xf\). For any \(v \in V\) and \(t < \infty\), let \(\Xf\vind{v}\tme{t}\) be the value the \(v\)-component of \(\Xf\) at time \(t\). Given a set \(A\subset V\) and an interval \(I \subset \mb{R}^+\), let \(\Xf\vind{A}\tmi{I}\) denote the path taken by the \(A\)-components of \(\Xf\) over \(\tmi{I}\). We denote the natural filtration of this process by \(\F\vpara{A}\tpara{t} \defeq \sigma \left(\Xf\vind{v}\tmi{[0,t]}\right)\). We will often be interested in the predictable sigma-algebra of the process given by \(\F\vpara{A}\tpara{t-} \defeq \bigvee_{s < t} \F\vpara{A}\tpara{s} = \sigma\left(\Xf\vind{v}\tmi{[0,t)}\right)\). Furthermore, when the evolution of \(\Xf\) with respect to its topology is clearly defined, but the specific interaction network of \(\Xf\) is not clear from context, we may write \(\Xf\gind{G}\) to represent the process \(\Xf\) with interaction network \(G\). Fix any \(v \in V,t \in \mb{R}^+\) and \(j \in \sta\setminus\{\Xf\vind{v}\tme{t}\}\). Then at time \(t\), we can define the jump rate \(\rate\gvpara{G}{v}\stpara{j}(G,t)\) to be the inverse of the expected time for \(\Xf\vind{v}\) to jump to \(\Xf\vind{v} + j\) time \(t\). It now becomes clear how we can define the interaction network. The interaction network is any graph \(G\) such that \(\Xf\) is a \(\sta^V\)-valued process and such that for every \(v \in V\),\(t\in \mb{R}^+\) and \(j \in \sta\), \(\rate\gvpara{G}{v}\stpara{j}(G,t)\) is \(\F\vpara{v}\tpara{t-}\)-measurable. 

\ind We can rigorously define this in the following manner. Let \(\Sm\) be the counting measure on \(\sta\). Let \(\leb\) be the Lebesgue measure on \(\mb{R}^2\). Let \(\{\poiss\poissv{v}:v \in V\}\) be a sequence of i.i.d. Poisson point processes on \(\sta\times \mb{R}^2\) with intensity \(\Sm\times \leb\). Let \(\Xf\tme{0}\) be an \(\sta^V\)-valued random variable. Let \(\Xf\tme{0}\) be some given \(\sta^V\)-valued random variable. Assume \(\rate\gvpara{G}{v}\stpara{j}(t)\) is \(\F\vpara{\cl{v}}\tpara{t-}\)-measurable for all \(v,j,t\) and that \(\rate\gvpara{G}{v}\stpara{j}:\mb{R}^+ \ra\mb{R}^+\) is an almost surely Borel-Measurable function. Consider the following SDE:

\begin{equation}
\Xf\gind{G}\vind{v}\tme{t} = \Xf\gind{G}\vind{v}\tme{0} + \int_{\sta}\int_{(0,t]\times (0,\infty)} i\mb{I}_{r \leq \rate\gvpara{G}{v}\stpara{i}(s)} \poiss\poissv{v}\left(dr,ds,di\right)
\label{p::Xf}
\end{equation}

Assuming equation \eqref{p::Xf} has a well-defined unique solution, this is the infinite-dimensional process whose marginals we are interested in.

\ind Let \(\m\) be the law of \(\Xf\). We are primarily interested in the marginals of \(\m\). For any \(A \subseteq V\), let \(\proj\vpara{A}(\Xf)\) map \(\Xf\) to an \(\sta^A\)-valued process defined by \((\proj\vpara{A}(\Xf))\vind{v} = \Xf\vind{v}\) for all \(v\in A\). Then the \(A\)-marginal of \(\m\) is given by the push-forward measure \(\proj\vpara{A}\psf(\m)\). We may also be interested in restricting the process to some finite time interval \([0,T)\). In this case, we define \((\proj\vpara{A}\tpara{T}(\Xf))\vind{v}\tme{t} = \Xf\vind{v}\tme{t}\) for \(v \in A\) and \(t \in [0,T)\). The corresponding push-forward measure is given by \(\proj\vpara{A}\tparapsf{t}(\m)\).

\subsubsection{Considering Simultaneous Jumps}
\label{sim::p}

The model described above is fairly general. However, it does not account for certain models such as the exclusion process in which multiple nodes of \(\Xf\) may update simultaneous with positive probability. This can be accounted for in the following manner. Let \(\Jmps \subseteq 2^V\) be the set of subsets of nodes of \(\Xf\) that can jump simultaneously with positive probability. We assume \(\Jmps\) is composed of finite sets. 

Let \tr{continue with this later. For now focus on the main argument. The idea here is to construct an interaction graph (kind of like I did for load-balancing earlier) which treats all simultaneous jumps as single nodes. Ideally this graph satisfies the conditions of general networks outlined later in the paper.}

\subsection{Assumptions}
\label{a::not}

The assumptions required for our results vary from extremely general to fairly restrictive.

\begin{assu}
\(G\) is a countable, connected, locally finite rooted graph.
\label{a::gbasics}
\end{assu}

All graphs considered in this paper will satisfy assumption \ref{a::gbasics}. Countability is required for obvious reasons. We restrict our attention to rooted graphs so that we can clearly describe local properties (this could also be achieved using ordinary graphs and choosing an arbitrary vertex, however working with rooted graphs simplifies matters greatly). We need the graphs to be locally finite so that all finite vertex sets have finite neighborhoods within the graph. \tr{Most of my results currently hold only for bounded degree graphs rather than locally finite. For now I will write up the results as if I have already extended everything, but I need to keep an eye out for this assumption when pulling results from the main paper.}

\ind \tr{Connectedness is not actually necessary for my results. For well-posedness, it suffices to prove that the process is well-defined on all components (reducing to the connected case). Local convergence automatically discounts everything except for the connected component on which the root appears, so we can assume everything is connected without loss of generality. The conditional independence property holds on the whole graph if it holds on each component (by independence of the components). The local approximation would require some tedious graph theoretic arguments, but we can essentially ignore all components that do not contain a vertex from the admissible set. I suspect there's some simple way to consider each separate component of the admissible set separately, but I would have to think about it some more. I include the assumption of connectedness because I don't want to get bogged down in arguments about connected components.}

\ind Next, we need some general assumptions under which the process \(\Xf\) is well-defined.

\begin{assu}
\(G\) satisfies assumption \ref{a::gbasics}. \(\Xf\gind{G}\) satisfies equation \eqref{p::Xf}, and 
\begin{enumerate}
\item 

\begin{equation}
\ex{\sup_v |\Xf\gind{G}\vind{v}\tme{0}|} < \infty
\label{a::bddstart}
\end{equation}

\item 

\begin{equation}
\sum_{i \in \mb{Z}}|i|\sup_{v \in V,t \in \mb{R}^+} \rate\gvpara{G}{v}\stpara{i}\tme{t} < \infty
\label{a::bddjmp}
\end{equation}
\end{enumerate}

Furthermore, \(\rate\gvpara{G}{v}\stpara{i}\tme{t}\) is \(\F\vpara{\cl{v}}\tpara{t-}\) measurable for all \(v\in V,i \in \sta, t \in \mb{R}^+\), and \(\rate\gvpara{G}{v}\stpara{i}\tme{t}: \mb{R}^+ \ra \mb{R}^+\) is almost surely Borel-measurable. \tr{Alternatively, \(\rate\gvpara{G}{v}\stpara{i}\tme{\cdot}\) is \(\{\F\vpara{\cl{v}}\tpara{\cdot}\}\)-predictable for all \(v \in V, i \in \sta\).}

Note: for clarity, I may occasionally use the notation, \(\rate\gvpara{G}{v}\stpara{i}(\Xf\gind{G}\vind{\cl{v}}\tmi{[0,t)}) \defeq \rate\gvpara{G}{v}\stpara{i}\tme{t}\).

\label{a::pbasics}
\end{assu}

\tr{Assumption \ref{a::pbasics} is subject to change. I'm just hypothesizing these conditions at the moment.} These equations simply ensure that \(\Xf\gind{G}\) does not explode in finite time. Hopefully this is enough to ensure that \(\Xf\gind{G}\) is the unique solution to equation \eqref{p::Xf}.

\ind The local equations (see section \iffalse\ref{}\fi) are derived through the application of numerous symmetries in the network and process. The best way to characterize these symmetries is in terms of isomorphisms.

\begin{defn}
Given two graphs \(G\) and \(G'\), an isomorphism is a bijection \(\phi: V \ra V'\) that satisfies \((\phi(u),\phi(v)) \in E'\) if and only if \((u,v) \in E\).
\label{a::iso}
\end{defn}

This notion can be extended to rooted graphs in the following manner:

\begin{defn}
A rooted isomorphism between two rooted graphs \(G\) and \(G'\) is an isomorphism \(\phi\) such that \(\phi(\root) = \root'\).
\label{a::riso}
\end{defn}

Symmetries naturally arise when we start looking at isomorphisms from a graph \(G\) to itself:

\begin{defn}
For any graph \(G\), a self-isomorphism \(\phi: V \ra V\) is an isomorphism from \(G\) to itself. Note: if \(G\) is a rooted graph, it is possible that \(\phi(\root) \neq \root\).
\label{a:siso}
\end{defn}

Of course, the most important point is that the symmetry extend to \(\Xf\).

\begin{defn}
Let \((G,\Xf\gind{G})\) be a process satisfying assumption \ref{a::pbasics}. \(\phi: V \ra V\) is said to be a symmetry of \(\Xf\gind{G}\) if,

\begin{enumerate}
\item \(\phi\) is a self-isomorphism of \(G\).

\item 

\begin{equation}
\rate\gvpara{G}{\phi(v)}\stpara{i}\left(\Xf\gind{G}\vind{\phi(\cl{v})}\tmi{[0,t)}\right) = \rate\gvpara{G}{v}\stpara{i}\left(\Xf\gind{G}\vind{\cl{v}}\tmi{[0,t)}\right) \te{ for all } v \in V,i \in \sta, t \in \mb{R}^+.
\end{equation}
\end{enumerate}
\label{a:Xsim}
\end{defn}

This is called a symmetry of \(\Xf\) for a simple reason:

\begin{prop}
Let \((G,\Xf\gind{G})\) satisfy assumption \ref{a::pbasics}, and suppose \(\phi\) is a symmetry of \(\Xf\gind{G}\). Suppose also that \(\Xf\gind{G}\vind{\phi(V)}\tme{0} \deq \Xf\gind{G}\vind{V}\tme{0}\). Then,

\[\Xf\gind{G}\vind{V} \deq \Xf\gind{G}\vind{\phi(V)},\]

where \(\phi(V)\) is treated as a permutation of \(V\).
\end{prop}
\begin{proof}
This proof assumes that assumption \ref{a::pbasics} is enough to ensure that \(\Xf\gind{G}\) is the unique solution to equation \eqref{p::Xf}. In that case, use the symmetries to show that equation \eqref{p::Xf} is invariant in law with respect to application of \(\phi\). \tr{(Refer to some of my older papers for such a proof.)}
\end{proof}

Now we can characterize the types of symmetries necessary for the local equations to be asymptotically exact.

\begin{assu}
Let \((G,\Xf\gind{G})\) satisfy assumption \ref{a::pbasics}. \(G\) is said to be admissible if there exists a finite set \(A \subset V\) such that,

\begin{itemize}
\item There exists a finite partition \(\{B_i\}_{i=1}^\ell\) of \(A^c\).

\item \(\dneigh{B_i} \subseteq A\) for all \(i\).

\item Let \(C_i = B_i\cap \neigh{A}\). For any \(i \neq j\), there exists a symmetry \(\phi_{i,j}\) such that \(\phi_{i,j}(A) = \phi_{i,j}(A)\), \(\phi_{i,j}(C_i) = C_j\) and \(\phi_{i,j}(\dneigh{B_i}) = \dneigh{B_j}\).

\item There exists a set \(D \subseteq A\) such that for each \(i\), there exists a self-isomorphism of \(G\), \(\phi_i\), such that \(\phi_i(C_i\cup \dneigh{B_i}) = D\).

\item If we let \(A^{(1)} = A\cup \left(\bigcup_{i=1}^\ell C_i\right)\). Then \(A^{(1)}\) is also admissible with partition \(\{B^{(1)}_i\}_{i=1}^{\ell'}\). Let \(C^{(1)}_i = B^{(1)}_i\cap \neigh{A^{(1)}}\). Then for any \(i \leq \ell\) and \(i' \leq \ell'\), there exists a self-isomorphism \(\phi^{(1)}_{i,i'}\) such that \(\phi^{(1)}_{i,i'}(C_i) = C^{(1)}_{i'}\) and \(\phi^{(1)}_{i,i'}(B_i\cap\neigh{A}) = B^{(1)}_{i'}\cap \neigh{A^{(1)}}\). As a result, \(D^{(1)} = D\). Notice that this is a recursive definition.
\end{itemize}

The set \(A\) is called an admissible set. \(\{B_i\}\) will be referred to as the set of branches of \(A\). \(\{C_i\}\) will be called the boundary nodes of \(A\). \(D\) is the template set.

\ind \tr{Right now I'm just adding assumptions whenever I need them (after making sure load-balancing satisfies them).}
\label{a::admissible}
\end{assu}

The definition of an admissible set is designed ensure a high enough level of symmetry for the arguments in the main paper to hold in this case as well. \tr{Interestingly enough, if this assumption suffices to prove the local approximation, then we will be able to understand the dynamics of a typical class of particles even when such particles are split into heterogeneous classes so long as they are distributed with a high level of symmetry. This will be especially useful if we consider random graphs later.}

\subsection{Examples}
\label{e::not}

\begin{itemize}
\item Ising/Potts Model on the regular tree.

\item Ising/Potts Model on the lattice (a counterexample).

\item \(k\)-neighbor JSQ routing for queues.

\item Asymmetric Exclusion Process.

\item Neuronal Models on tree-like structures.
\end{itemize}

\section{Well-Posedness and Convergence}
\label{WP}

Well-posedness is simple.

\begin{thms}
If assumptions \ref{a::gbasics} and \ref{a::pbasics} hold, then equation \eqref{p::Xf} has a unique strong solution.
\label{WP::WP}
\end{thms}
\begin{proof}
\tr{TODO}
\end{proof}

Often, the network of interest is a large, finite network. To analyze such a network, we consider an approximation at infinity. To do this, we need some appropriate notions of convergence.

\begin{defn}
The \(k\)-truncation of a rooted graph \(G = (V,E,\root)\) is the graph \(G\trnc{k}=(V\trnc{k},E\trnc{k},\root)\) given by,

\[V\trnc{k} = \{v \in V: d_G(v,\root) \leq k\} \te{ and } E\trnc{k} = \{(u,v) \in E: u,v \in V\trnc{k}\}.\]
\label{WP::trunc}
\end{defn}

\begin{defn}
Let \(G\indx{n} = (V\indx{n},E\indx{n},\root\indx{n})\) be a sequence of rooted graphs. We say \(G\indx{n} \ra G = (V,E,\root)\) locally if for every \(k\in \mb{N}\), there exists an \(n_k \in \mb{N}\) such that for all \(n \geq n_k\), there exists a graph isomorphism \(\psi_{n,k}: V\indx{n}\trnc{k} \ra V\trnc{k}\), where \(V\indx{n}\trnc{k}\) is the \(k\)th truncation of \(V\indx{n}\) and \(V\trnc{k}\) is the \(k\)th truncation of \(V\).
\label{WP::locconv}
\end{defn}

\begin{defn}
Let \(G^n\) be a sequence of graphs converging locally to a graph \(G\) that satisfies assumption \ref{a::gbasics}. Let \(\Xf\indx{n}\) be a \(\sta^{V\indx{n}}\)-valued process. Let \(\Xf\) be a \(\sta^V\)-valued process. 

\ind For every \(k\in \mb{N}\), \(n \geq n_k\) and \(v \in V\trnc{k}\), let \(\Xf\indx{n,k}\vind{v} = \Xf\indx{n}\vind{\psi_{n,k}^{-1}(v)}\). Then \(\Xf^n\) converges locally weakly to \(\Xf\) if for every \(k \in \mb{N}\),

\[\Xf^{n,k} \os{n\ra\infty}{\Rightarrow} \Xf_{V_k}.\]
\label{WP::locweak}
\end{defn}

\begin{thms}
Let \(G\indx{n}\) be a sequence of rooted graphs converging locally to \(G\), which satisfies assumption \ref{a::gbasics}. Suppose \(\Xf\gind{G}\) satisfies assumption \ref{a::pbasics}. Then \(\Xf\gind{G\indx{n}} \ra \Xf\gind{G}\) locally weakly.
\label{WP::lblocweak}
\end{thms}
\begin{proof}
\tr{I should be calling this a conjecture for now.}
\end{proof}

\section{Conditional Independence}
\label{CI}

The results from the main paper are almost sufficient. I just need to extend them to the infinite state space case and to work with non-Markovian processes. Only the arguments of Lemma 4.3 from the main paper rely on the process having a finite state space and Markov structure, so I only have to redo that one.

\begin{thms}
Suppose \((G,\Xf\gind{G})\) satisfy assumptions \ref{a::gbasics} and \ref{a::pbasics}. Let \(U \subseteq V\) and let \(W = V\setminus \ov{\ov{U}}\). Let \(R = \dneigh{U}\) and suppose \(|R| < \infty\). Then for all \(t < \infty\),

\begin{equation}
\Xf\vind{U}\tmi{[0,t)}\perp \Xf\vind{W}\tmi{[0,t)}|\Xf\vind{R}\tmi{[0,t)}
\label{CI::CIeqn}
\end{equation}

\label{CI::CI}
\end{thms}
\begin{proof}
I've proved version of this theorem in the main paper with stronger assumptions (Theorem 3.1) (\(\Xf\gind{G}\) is assumed to be Feller, \(G\) is assumed to be of bounded degree instead of locally-finite and \(\sta\) is assumed to be finite instead of countable). However, I suspect that proving this will only require some modifications to Lemma 4.3 from the main paper.
\end{proof}
\section{Statement of the Local Approximation}
\label{Main}

Let \(G\) be an admissible graph with admissible set \(A\), partitions \(\{B_i\}_{i=1}^\ell\), boundary set \(\{C_i\}_{i=1}^\ell\) and template set \(D\).

\begin{assu}
Let \(\Xf\) be a c\`adl\`ag \(\sta^V\)-valued process with some bound on each node's total jump rate and some bound to prevent any node from exploding in finite time. Define,

\begin{equation}
\Xf\vind{v}\tme{t} = \Xf\vind{v}\tme{0} + \sum_{k \in \sta}\int_{(0,t]}\int_{(0,C]} k\mb{I}_{r\leq r_v^k(\Xf\vind{\cl{v}}\tme{s-})} \poiss\poissv{v}(dr,ds).
\label{Main::full}
\end{equation}

where \(\poiss\poissv{v}\) is a unit rate Poisson process on \(\mb{R}^2\), and \(0 \leq r_v^k \leq C\) and \(r_v^k\) satisfies some other conditions (measurability and symmetry).
\label{Main::Xassu}
\end{assu}


\begin{thms}
Suppose assumption \ref{Main::Xassu} holds. Let \(\intA = \{v \in A: \neigh{v} \subseteq A\}\). Consider the following equations:

\begin{align}
\Xg\vind{v}\tme{t} &= 
\begin{cases}
\Xg\vind{v}\tme{t} + \sum_{k \in \sta} \int_{[0,t)}\int_{(0,C]} k\mb{I}_{r\leq r^k_v(\Xg\vind{\cl{v}}\tme{s-})}\,\poiss\poissv{v}(dr,ds) & \te{ if } v \in \intA\\
\Xg\vind{v}\tme{0} + \sum_{k \in \sta} \int_{[0,t)}\int_{(0,C]} k\mb{I}_{r\leq \alt{r}^k_v(\Xg\vind{\cl{v}\cap A}\tmi{[0,s)}}\,\poiss\poissv{v}(dr,ds) &\te{ otherwise}
\end{cases}\label{Main::local}\\
\alt{r}^k_v(x_{\cl{v}\cap A}[0,t)) &= \exmu{\Xg \sim \mu}{r^k_{\phi_i(v)}(\Xg\vind{A}\tme{t-})|\Xg\vind{\phi_i(\cl{v}\cap A)}\tmi{[0,t)} = x\vind{\cl{v}\cap A}\tmi{[0,t)}}\label{Main::CI}\\
\mu &= \law(\Xg).\label{Main::fixed}
\end{align}

Equations \eqref{Main::local}, \eqref{Main::CI} and \eqref{Main::fixed} have a unique weak solution in law, and the law of this solution is the marginal distribution of \(\law(\Xf)\) on \(\sta^A\).
\end{thms}

\section{Proof of Existence}
\label{Ex}

Just like in the regular tree case, the idea of the proof is to convert \(\Xf\) into a point process and use a well-known filtration result for point processes.

\ind In this section, we assume \(G\) is admissible. Let \(A,\{B_i\}_{i=1}^\ell, \{C_i\}_{i=1}^\ell\) and \(D\) be defined as in definition \ref{abs::admissible}.

\ind Let \(W\subseteq V\). \(\proj^W_T\) map a c\`adl\`ag path on \(\sta^U\) to its component parts on \(\sta^W\) for any \(U \supseteq W\). That is, if \(\Xg\) is a c\`adl\`ag, \(\sta^U\)-valued process, then \((\proj^W_T(\Xg))\vind{v} = \Xg\vind{v}\) for all \(v \in W\). The \(T\) denotes the time interval on which \(\proj^W_T(\Xg)\) is defined.

\begin{defn}
Let \(U\subseteq V\) and suppose \(\Xg\) is a \(\sta^U\)-valued c\`adl\`ag stochastic process adapted to its own natural filtration. Fix some \(0 < T < \infty\). Then define the marked point process \(\pmap(\Xg)\) as a random measure on \((0,T) \times \sta^U\) defined by,

\[\pmap(\Xg)(\{(\rt,\mark)\}) = \begin{cases}
1 &\te{ if } \Xg\tme{\rt} - \Xg\tme{\rt-} = \mark\\
0 &\te{ otherwise}
\end{cases}.\]

Similarly, for \(W \subseteq U\), \(\pmap^W(\Xg) = \pmap\left(\proj^W_{T}(\Xg)\right)\). For any \(v\in U\), \(\pmap^{v}(\Xg)\) is a random measure on \((0,T) \times \sta\) given by,

\[\pmap^{v}(\Xg)(\{(\rt,\mark)\}) = \begin{cases}
1 &\te{ if } \Xg\vind{v}\tme{\rt} - \Xg\vind{v}\tme{\rt-} = \mark\\
0 &\te{ otherwise}
\end{cases},\]

and

\[\pmap^{v}(\Xg)(\{(\rt,\mark): \Xg\vind{v}\tme{\rt} - \Xg\vind{v}\tme{\rt-} \neq \mark\}) = 0.\]
\label{Ex::pmap}
\end{defn}

\begin{assu}
Let \(A\subseteq U \subseteq V\). Let \(\Xg\) be an \(\sta^U\)-valued c\`adl\`ag stochastic process on \((0,T)\). For each \(v\in U\), \(\pmap^{v}(\Xg)\) has an \(\Xg\)-predictable intensity, \(\ratee^{v}\), and there exists a constant \(C < \infty\) such that \(\sup_{v\in U} \ratee^{v} \leq C\). Furthermore, \(f: \sta^{|A|}\times \sta\ra[0,C]\) is some function.
\label{Ex::Eassu}
\end{assu}


\begin{lem}
Let \(\rp\) be an \(\F\)-adapted marked point process with \(\F\)-predictable intensity \(\ratee\) with respect to the reference measure \(\ell\) on its mark space. For all \(t \in [0,T]\), let \(\sigma(\rp_{t}) \subseteq \alt{\F}_{t}\subset \F_{t}\) define a subfiltration of \(\F\) containing the history of \(\rp\). If there exists an \(\ell\times \pr\)-almost sure left-continuous modification of \(\cratee(t,\mark) := \ex{\ratee(t,\mark)|\alt{\F}_{t-}}\) with respect to \(t\), then \(\cratee\) is the \(\alt{\F}\)-predictable intensity of \(\rp\).
\label{Ex::filtering}
\end{lem}

\begin{proof}
This is Lemma 5.3 from the main paper.
\end{proof}

Now we need a simple method to know when the conditional expectation has a left-continuous modification.


\begin{lem}
Suppose Assumption \ref{Ex::Eassu} holds. Let \(\mu = \law(\Xg)\) and \(v \in A\) with \(\neigh{v}\setminus A \subset B_i\). For any \(y \in \sta\), let

\[\ratee^{v}_{t}(y) := \ratee(\Xg\vind{\cl{v}\cap A}\tmi{[0,t)},y) = \exmu{\Xh\sim \mu}{f(\Xh\vind{A}\tme{t-},y)|\Xh\vind{\phi_i(A\cap\cl{v})}\tmi{[0,t)} = \Xg\vind{A\cap\cl{v}}\tmi{[0,t)}}.\]

Recall \(\phi_i\) was defined in definition \ref{abs::admissible}. Then \(\ratee^{v}_{t}(y)\) has an almost surely left-continuous modification on \([0,T)\) with respect to \(t\) for every \(v \in A\) and \(\ell\)-almost every \(y \in \sta\). \tr{\(\ell\) will be properly defined when I have a reasonable set of assumptions on the jump rates (assumption \ref{Main::Xassu})}.
\label{Ex::leftmod}
\end{lem}
\begin{proof}
Lemma 5.4 of the main paper is extremely similar to this Lemma. I can go back and generalize that Lemma to apply to this situation later so that this becomes a direct application of lemma 5.4 of the main paper.
\end{proof}
The remainder of the proof is fairly straight-forward. I'll finish it later.

\section{Proof of Uniqueness}

Let \((\mu,\Xg)\) be a solution to the local equations (\eqref{Main::local}-\eqref{Main::fixed}). I'm stuck trying to compute the Marginal distribution of \(\Xg\vind{\dneigh{B_i}}\) for a fixed \(i\) in terms of \(\mu\). The goal should be to use filtering theory to write a Poisson SDE with jump rates dependent upon conditional expectations of the past. However, I had some technical issues.

\ind This lemma is necessary because we cannot expand the process without conditioning on that marginal. We cannot condition on that marginal without understanding its law.

\ind I'll explain the difficulties in person tomorrow.
\newpage
\bibliographystyle{plain}
\bibliography{weekly_refs}
\end{document}
